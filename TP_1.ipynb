{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 1\n",
    "\n",
    "## Predict whether a mammogram mass is benign or malignant\n",
    "\n",
    "We'll be using the \"mammographic masses\" public dataset.\n",
    "\n",
    "This data contains 961 instances of masses detected in mammograms, and contains the following attributes:\n",
    "\n",
    "\n",
    "   1. BI-RADS assessment: 1 to 5 (ordinal)  \n",
    "   2. Age: patient's age in years (integer)\n",
    "   3. Shape: mass shape: round=1 oval=2 lobular=3 irregular=4 (nominal)\n",
    "   4. Margin: mass margin: circumscribed=1 microlobulated=2 obscured=3 ill-defined=4 spiculated=5 (nominal)\n",
    "   5. Density: mass density high=1 iso=2 low=3 fat-containing=4 (ordinal)\n",
    "   6. Severity: benign=0 or malignant=1 (binominal)\n",
    "   \n",
    "BI-RADS is an assesment of how confident the severity classification is; it is not a \"predictive\" attribute and so we will discard it. The age, shape, margin, and density attributes are the features that we will build our model with, and \"severity\" is the classification we will attempt to predict based on those attributes.\n",
    "\n",
    "Although \"shape\" and \"margin\" are nominal data types, which sklearn typically doesn't deal with well, they are close enough to ordinal that we shouldn't just discard them. The \"shape\" for example is ordered increasingly from round to irregular.\n",
    "\n",
    "A lot of unnecessary anguish and surgery arises from false positives arising from mammogram results. \n",
    "\n",
    "Build a better way to interpret them through supervised machine learning.\n",
    "\n",
    "## Your assignment\n",
    "\n",
    "Apply Artificial Neural Network supervised machine learning techniques to this data set and validate it by applying K-Fold cross validation (K=10).\n",
    "\n",
    "The data needs to be cleaned; many rows contain missing data, and there may be erroneous data identifiable as outliers as well.\n",
    "\n",
    "Many optimization techniques provide the means of \"hyperparameters\" to be tuned (e.g. Genetic Algorithms). Once you identify a promising approach, see if you can make it even better by tuning its hyperparameters.\n",
    "\n",
    "Below it's described the set of steps that outline the development of this project, with some guidance and hints. If you're up for a real challenge, try doing this project from scratch in a new, clean notebook!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's begin: prepare your data\n",
    "\n",
    "Start by importing the mammographic_masses.data.txt file into a Pandas dataframe (hint: use read_csv) and take a look at it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from statistics import *\n",
    "from sklearn.model_selection import train_test_split\n",
    "data = pd.read_csv(\"mammographic_masses.data.txt\", sep=\",\", header=None, na_values = '?').astype('float64')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure you use the optional parmaters in read_csv to convert missing data (indicated by a ?) into NaN, and to add the appropriate column names (BI_RADS, age, shape, margin, density, and severity):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.columns = [\"BI-RADS\", \"Age\", \"Shape\", \"Margin\", \"Density\", \"Severity\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate whether the data needs cleaning; your model is only as good as the data it's given. Hint: use describe() on the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are quite a few missing values in the data set. Before we just drop every row that's missing data, let's make sure we don't bias our data in doing so. Does there appear to be any sort of correlation to what sort of data has missing fields? If there were, we'd have to try and go back and fill that data in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import seaborn as sb\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "#sb.pairplot(data,hue='Severity')\n",
    "print(data.isna().sum(axis=0))\n",
    "sb.heatmap(data.isnull(), cbar=False)\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the missing data seems randomly distributed, go ahead and drop rows with missing data. Hint: use dropna()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Next you'll need to convert the Pandas dataframes into numpy arrays that can be used by scikit_learn. Create an array that extracts only the feature data we want to work with (age, shape, margin, and density) and another array that contains the classes (severity). You'll also need an array of the feature name labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data.drop(['BI-RADS','Severity'], axis=1)\n",
    "Y = data['Severity']\n",
    "X=X.reset_index(drop=True)\n",
    "Y=Y.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = data.columns.drop(['BI-RADS','Severity'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some of our models require the input data to be normalized, so go ahead and normalize the attribute data. Hint: use preprocessing.StandardScaler()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X)\n",
    "scaled_features = scaler.fit_transform(X)\n",
    "df_feat = pd.DataFrame(scaled_features,columns=names)\n",
    "X=df_feat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Networks\n",
    "\n",
    "You can use Tensorflow to set up a neural network with 1 binary output neuron and see how it performs. Don't be afraid to run a large number of epochs to train the model if necessary. As a bonus, try to optimize this model's hyperparameters using GA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report,confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "age = tf.feature_column.numeric_column(\"Age\")\n",
    "shape = tf.feature_column.numeric_column(\"Shape\")\n",
    "margin = tf.feature_column.numeric_column(\"Margin\")\n",
    "density = tf.feature_column.numeric_column(\"Density\")\n",
    "\n",
    "feat_cols = [age,shape,margin,density]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions for hyperparameters adjustment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Função que cria a população inicial\n",
    "#parameters=[learning_rate,num_nodes,num_hidden_l,activation_fun]\n",
    "def create_new_population ():\n",
    "    \n",
    "    population=[]\n",
    "    \n",
    "    for i in range(10):\n",
    "        cromo=[]\n",
    "        cromo.append(np.random.uniform(low=10**-2, high=10**-1))\n",
    "        cromo.append(np.random.choice([1, 2, 4, 8, 16, 32, 64, 128, 256]))\n",
    "        cromo.append(np.random.randint(low=1, high=20))\n",
    "        cromo.append(np.random.randint(low=0, high=2))\n",
    "        population.append(cromo)\n",
    "        \n",
    "    return np.array(population)\n",
    "        \n",
    "\n",
    "\n",
    "#Função genérica que atualiza os argumentos do classifier\n",
    "#parameters=[learning_rate,num_nodes,num_hidden_l,activation_fun]\n",
    "def update_classifier_parameters (parameters):\n",
    "    h_u=[]\n",
    "    for i in range(int(parameters[2])):\n",
    "        h_u.append(parameters[1])\n",
    "    \n",
    "    if((parameters[3]) == 0): a_f = tf.nn.softmax\n",
    "    if((parameters[3]) == 1): a_f = tf.nn.relu\n",
    "    if((parameters[3]) == 2): a_f = tf.nn.leaky_relu\n",
    "    \n",
    "    classifier = tf.estimator.DNNClassifier(hidden_units=h_u,\n",
    "                                           n_classes=2,\n",
    "                                           feature_columns=feat_cols,\n",
    "                                           #model_dir='C:\\\\Users\\\\jose\\\\Desktop\\\\RNAmodel',\n",
    "                                           activation_fn=a_f,\n",
    "                                           dropout=0.5,\n",
    "                                           optimizer=tf.train.AdamOptimizer(\n",
    "                                              learning_rate=parameters[0]\n",
    "                                           ))\n",
    "    print(h_u)\n",
    "    print(type(a_f))\n",
    "    return classifier\n",
    "\n",
    "def select_mating_pool(pop, fitness, parents_fitness, num_parents):\n",
    "    # Selecting the best individuals in the current generation as parents for producing the offspring of the next generation.\n",
    "    parents = np.empty((num_parents, pop.shape[1]))\n",
    "    #parents_fitness=[] - strangely not working good\n",
    "    for parent_num in range(num_parents):\n",
    "        #save fitness values of best parents\n",
    "        parents_fitness.append(np.max(fitness))\n",
    "        #save best parents\n",
    "        max_fitness_idx = np.where(fitness == np.max(fitness))\n",
    "        max_fitness_idx = max_fitness_idx[0][0]\n",
    "        parents[parent_num, :] = pop[max_fitness_idx, :]\n",
    "        fitness[max_fitness_idx] = -99999999999\n",
    "        \n",
    "    return parents\n",
    "\n",
    "\n",
    "def crossover(parents, offspring_size):\n",
    "    offspring = np.empty(offspring_size)\n",
    "    # The point at which crossover takes place between two parents. Usually it is at the center.\n",
    "    crossover_point = np.uint8(offspring_size[1]/2)\n",
    "\n",
    "    for k in range(offspring_size[0]):\n",
    "        # Index of the first parent to mate.\n",
    "        parent1_idx = k%parents.shape[0]\n",
    "        # Index of the second parent to mate.\n",
    "        parent2_idx = (k+1)%parents.shape[0]\n",
    "        # The new offspring will have its first half of its genes taken from the first parent.\n",
    "        offspring[k, 0:crossover_point] = parents[parent1_idx, 0:crossover_point]\n",
    "        # The new offspring will have its second half of its genes taken from the second parent.\n",
    "        offspring[k, crossover_point:] = parents[parent2_idx, crossover_point:]\n",
    "    return offspring\n",
    "\n",
    "\n",
    "def mutation(offspring_crossover):\n",
    "    # Mutation changes a single gene in each offspring randomly.\n",
    "    for idx in range(offspring_crossover.shape[0]):\n",
    "        \n",
    "        # Select which gene to mutate\n",
    "        select_gene = np.random.randint(low=0, high=4)\n",
    "        \n",
    "        if(select_gene == 0):\n",
    "            #Learning rate mutation\n",
    "            random_value = np.random.uniform(low=10**-2, high=10**-1)\n",
    "            offspring_crossover[idx,0] = random_value\n",
    "        if(select_gene == 1):\n",
    "            #num_nodes_per_layer mutation\n",
    "            random_value = np.random.choice([1, 2, 4, 8, 16, 32, 64, 128, 256])\n",
    "            offspring_crossover[idx,1] = random_value\n",
    "        if(select_gene == 2):\n",
    "            #num_hidden_layers\n",
    "            random_value = np.random.randint(low=1, high=20)\n",
    "            offspring_crossover[idx,2] = random_value\n",
    "        if(select_gene == 3):\n",
    "            #activation function mutation\n",
    "            random_value = np.random.randint(low=0, high=2)\n",
    "            offspring_crossover[idx,3] = random_value\n",
    "            \n",
    "    return offspring_crossover\n",
    "              "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data split using 10 folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kf = KFold(n_splits=10)\n",
    "kf.get_n_splits(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build and validate ANN for a given chromosome"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_create_folds(cromossoma):\n",
    "    fold = 0\n",
    "    all_test_samples=[]\n",
    "    predicted_labels=[]\n",
    "    scores=[]\n",
    "    \n",
    "    classifier = update_classifier_parameters(cromossoma)\n",
    "\n",
    "    for train_index, test_index in kf.split(X):\n",
    "        fold+=1\n",
    "        print(\"Fold#{}\".format(fold))\n",
    "\n",
    "        X_train = X.values[train_index]\n",
    "        y_train = Y[train_index]\n",
    "        X_test = X.values[test_index]\n",
    "        y_test = Y[test_index]\n",
    "\n",
    "        X_train_df = pd.DataFrame(X_train,columns=names,index=train_index)\n",
    "        X_test_df = pd.DataFrame(X_test,columns=names,index=test_index)\n",
    "\n",
    "        #defining input function to feed the classifier with training data\n",
    "        input_func = tf.estimator.inputs.pandas_input_fn(x=X_train_df,\n",
    "                                                         y=y_train,\n",
    "                                                         batch_size=20,\n",
    "                                                         shuffle=True)\n",
    "   \n",
    "        #train the model\n",
    "        classifier.train(input_fn=input_func,steps=500)\n",
    "\n",
    "        #defining input function to feed the classifer with testing data\n",
    "        pred_fn = tf.estimator.inputs.pandas_input_fn(x=X_test_df,\n",
    "                                                      batch_size=len(X_test_df),\n",
    "                                                      shuffle=False)\n",
    "\n",
    "        #make predictions based on testing data\n",
    "        note_predictions = list(classifier.predict(input_fn=pred_fn))\n",
    "\n",
    "        #extract the labels\n",
    "        final_preds=[]\n",
    "        for pred in note_predictions:\n",
    "            final_preds.append(pred['class_ids'][0])\n",
    "\n",
    "        #Adding all accuracy values to an array\n",
    "        acc = accuracy_score(y_test,final_preds)\n",
    "        \n",
    "        scores.append(acc)\n",
    "        print(\"Fold-{}\".format(fold),\"Accuracy#{}\".format(acc))\n",
    "\n",
    "        \n",
    "   \n",
    "    return (scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "## Hyperparameters otimization using Genetic Algorithm "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "new_population = create_new_population()\n",
    "print(new_population)\n",
    "num_parents_mating = 5\n",
    "num_generations = 20\n",
    "#number of genes for each chromosome\n",
    "num_genes = 4 \n",
    "#number of chromosomes for each population\n",
    "num_chromosomes = 10 \n",
    "pop_size=(num_chromosomes,num_genes)\n",
    "#fitness values for each chromosome for the current generation\n",
    "fitness_values = []\n",
    "#fitness vaalues for each chromosome of the last generation\n",
    "last_fitness_values = []\n",
    "gen = 0\n",
    "cromo = 0\n",
    "parents=[]\n",
    "#Parents fitness so we do not repeat calculations on parents\n",
    "parents_fitness = []\n",
    "\n",
    "performances=[]\n",
    "hiperparametros=[]\n",
    "\n",
    "\n",
    "for generation in range(num_generations):\n",
    "    gen+=1\n",
    "    cromo = 0\n",
    "    best_perf_per_gen = -1\n",
    "    \n",
    "    for cromossoma in new_population:\n",
    "        cromo+=1\n",
    "        score=-1\n",
    "        parentNumber=0\n",
    "        \n",
    "        # If it's a known chromosome we dont need to train the ANN again\n",
    "        # Skips the first generation because we didnt select the parents yet\n",
    "        for savedCromo in parents:\n",
    "            parentNumber+=1\n",
    "            if (np.array_equal(cromossoma,savedCromo)):\n",
    "                score = parents_fitness[parentNumber-1]\n",
    "                print(savedCromo, \"CROMOSSOMA CONHECIDO\")\n",
    "                print(score, \"SCORE CONHECIDO\")\n",
    "\n",
    "        \n",
    "        #If it's a new chromosome we need to train the ANN in order to get the accuracy\n",
    "        if (score < 0):\n",
    "            scores = classify_create_folds(cromossoma)\n",
    "            score = sum(scores)/len(scores)\n",
    "            \n",
    "        print(\"Generation-{}\".format(gen),\"Cromossoma-{}\".format(cromo),\"scored\",score)\n",
    "        #Keep the scores in fitness_values\n",
    "        fitness_values.append(score)\n",
    "        \n",
    "        #getting the best hyperparameters per generation to check the evolution at the end\n",
    "        if(best_perf_per_gen < score):\n",
    "            best_perf_per_gen = score\n",
    "            best_cromo_per_gen = cromossoma\n",
    "           \n",
    "        \n",
    "        \n",
    "        print(cromossoma)\n",
    "        \n",
    "    performances.append(best_perf_per_gen)\n",
    "    hiperparametros.append(best_cromo_per_gen)\n",
    "   \n",
    "    \n",
    "    print(performances,\"MELHORES DE CADA GERAÇÃO\")\n",
    "    print(hiperparametros,\"MELHORES ACCURACIES DE CADA GERAÇÃO\")\n",
    "    #We store last generation in other array because fitness_values is changed by the selec_mating_pool\n",
    "    if(gen == num_generations):\n",
    "        for i in fitness_values:\n",
    "            last_fitness_values.append(i)\n",
    "            \n",
    "    print(last_fitness_values,\"LAST_FITNESS_VALUES\")\n",
    "    parents_fitness=[]\n",
    "    parents = select_mating_pool(new_population,fitness_values,parents_fitness,num_parents_mating)\n",
    "    #print(parents)\n",
    "    # Generating next generation using crossover.\n",
    "    offspring_crossover = crossover(parents,\n",
    "                                        offspring_size=(pop_size[0]-parents.shape[0], num_genes))\n",
    "\n",
    "\n",
    "    # Adding some variations to the offspring using mutation.\n",
    "    offspring_mutation = mutation(offspring_crossover)\n",
    "\n",
    "    # Creating the new population based on the parents and offspring.\n",
    "    new_population[0:parents.shape[0], :] = parents\n",
    "    new_population[parents.shape[0]:, :] = offspring_mutation #mudar para offspring_mutation quando mutation funcionar\n",
    "    \n",
    "    #Reset fitness_values\n",
    "    fitness_values=[]\n",
    "\n",
    "#Getting the best solution\n",
    "print(new_population)\n",
    "best_solution = new_population[last_fitness_values.index(np.max(last_fitness_values))]\n",
    "print(\"The best hyperparameters obtained are\",best_solution,\"with an accuracy of\",np.max(last_fitness_values))\n",
    "\n",
    "    \n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Debug stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "new_population = create_new_population()\n",
    "print(new_population)\n",
    "fitness_values=[]\n",
    "for i in range(10):\n",
    "    fitness_values.append(np.random.uniform(low=0, high=1))\n",
    "\n",
    "print(fitness_values.index(np.max(fitness_values)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accs=[0.7951807228915662, 0.7963855421686746, 0.7963855421686746, 0.8012048192771084, 0.8012048192771084, 0.8012048192771084, 0.8084337349397591, 0.8084337349397591, 0.8084337349397591, 0.8096385542168674, 0.8120481927710843, 0.8120481927710843, 0.8120481927710843, 0.8120481927710843, 0.8120481927710843, 0.8120481927710843, 0.8120481927710843, 0.8120481927710843, 0.8120481927710843, 0.8132530120481928]\n",
    "gen=[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20]\n",
    "plt.plot(gen,accs,color='g')\n",
    "plt.xlabel('Generations')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Accuracy improvement through generations')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
